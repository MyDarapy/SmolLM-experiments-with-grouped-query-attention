### Improving Sub-billion Scale LLM Design Experiments
Some of the techniques used in the LLM pretraining design include:
- Embedding Sharing 
- Grouped Query Design
- SwiGLU Activations for the Multi Perceptron Layer
- Intermidate blockwise weight sharing
  
<img width="544" alt="chrome_7KTClIuUnl" src="https://github.com/user-attachments/assets/3a0738f1-bd1c-49d5-8ea3-ab8276f34ef0">
